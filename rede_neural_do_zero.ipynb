{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXoHcLv16LPyy+2r8nTD3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusmarinhob/dio-ml-with-python/blob/main/rede_neural_do_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Kh-po8TGAHLO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nf\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo a conversão de imagem para Tensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Carrega a parte de treino do dataset\n",
        "trainset = datasets.MNIST('./MNIST_data', download=True, train=True, transform=transform)\n",
        "\n",
        "# Cria um buffer para pegar os dados por partes\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Carrega a parte de validação\n",
        "valset = datasets.MNIST('./MNIST_data', download=True, train=False, transform=transform)\n",
        "\n",
        "# Cria um buffer para pegar os dados por partes\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "BV4pIhaBAwk6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando uma imagem da base de dados\n",
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "vYtVu4JWBvSp",
        "outputId": "a7d2f95c-6fa6-48f3-81f1-b5d33e2d478e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2ea9df4050>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar as dimensões do tensor de cada imagem\n",
        "print(imagens[0].shape)\n",
        "\n",
        "# Verificar as dimensões do tensor de cada etiqueta\n",
        "print(etiquetas[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Xka6G9DaJC",
        "outputId": "80379deb-1f4c-4ff7-8858-c9e341b562ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128)  # Camada de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64)     # Camada interna 1, 128 neurônios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10)      # Camada interna 2, 64 neurônios que se ligam a 10\n",
        "    \n",
        "    # para a camada de saída não é necessário definir nada, pois só precisamos pegar o output da camanda interna 2\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.linear1(X))           # Função de ativação da camada de entrada para a camada interna 1\n",
        "    X = F.relu(self.linear2(X))           # Função de ativação da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3(X)                   # Função de ativação da camada interna 2 para a camada de saída, nesse caso f(X)=X\n",
        "    return F.log_softmax(X, dim=1)        # Dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "FOBKQodGD1Uf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5)   # Define a política de atualização dos pesos e da bias\n",
        "  inicio = time()                                                      # Timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLLoss()                                              # Defininfo o critério para calcular a perda\n",
        "  EPOCHS = 10                                                          # Número de epochs que o algoritmo rodará\n",
        "  modelo.train()                                                       # Ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0                                                # Inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "      imagens = imagens.view(imagens.shape[0], -1)                     # Convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compatíveis com a...\n",
        "      otimizador.zero_grad()                                           # Zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device))                              # Colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device))       # Calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward()                                     # Backpropagation a partir da perda\n",
        "\n",
        "      otimizador.step()                                                # Atualizando os pesos e a bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item()                      # Atualização da perda acumulada\n",
        "\n",
        "\n",
        "    else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "\n",
        "  print(\"\\nTempo de treino (em minutos) =\", (time()-inicio())/60)"
      ],
      "metadata": {
        "id": "mLUumOk_Fpxh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0,0\n",
        "  \n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784)\n",
        "      # desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # output do modelo em escala logaritmica\n",
        "\n",
        "\n",
        "      ps = torch.exp(logps) # converte output para escala normal(lembrando que é um tensor)\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) # converte o tensor em um número, no caso, o número que o modelo previu como correto\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "\n",
        "      if etiqueta_certa == etiqueta_pred: # compara a previsão com o valor correto\n",
        "        conta_corretas += 1\n",
        "      \n",
        "      conta_todas += 1\n",
        "\n",
        "  print(\"Total de imagens testadas = \", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "nOUOvTVgIMBx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo() # inicializa o modelo\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # modelo rodará na GPU se possível\n",
        "\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwFa6WAFLr-Z",
        "outputId": "fbb589ca-0ef2-40e2-fddd-c3e427051e37"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_-F_4ncMPmJ"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}